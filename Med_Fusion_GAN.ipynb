{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ybK2SZ02VrLs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, size, channels):\n",
        "    # Resize the data\n",
        "    data = np.array([np.resize(image, (size, size, channels)) for image in data])\n",
        "\n",
        "    # Normalize the data\n",
        "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "    return data\n",
        "\n",
        "def data_loader(data_dir_mri, data_dir_ct, batch_size, image_size, label_size, c_dim):\n",
        "    \"\"\"\n",
        "    Data loader function to load MRI, CT images, and their corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        data_dir_mri (str): Path to the directory containing MRI data.\n",
        "        data_dir_ct (str): Path to the directory containing CT data.\n",
        "        batch_size (int): Batch size for training.\n",
        "        image_size (int): Size of the input images.\n",
        "        label_size (int): Size of the label images.\n",
        "        c_dim (int): Number of channels in the input images.\n",
        "\n",
        "    Yields:\n",
        "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: A batch of MRI images, MRI labels, CT images, and CT labels.\n",
        "    \"\"\"\n",
        "    # Open HDF5 files\n",
        "    mri_file = h5py.File(data_dir_mri, 'r')\n",
        "    ct_file = h5py.File(data_dir_ct, 'r')\n",
        "\n",
        "    # Get the dataset sizes\n",
        "    mri_data_size = mri_file['data'].shape[0]\n",
        "    ct_data_size = ct_file['data'].shape[0]\n",
        "    data_size = min(mri_data_size, ct_data_size)\n",
        "\n",
        "    # Create shuffled indices\n",
        "    indices = np.random.permutation(data_size)\n",
        "\n",
        "    # Iterate over the data in batches\n",
        "    for batch_start in range(0, data_size, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, data_size)\n",
        "        batch_indices = indices[batch_start:batch_end]\n",
        "\n",
        "        # Load MRI data\n",
        "        mri_images = mri_file['data'][batch_indices, :, :, :]\n",
        "        mri_labels = mri_file['labels'][batch_indices, :, :, :]\n",
        "\n",
        "        # Load CT data\n",
        "        ct_images = ct_file['data'][batch_indices, :, :, :]\n",
        "        ct_labels = ct_file['labels'][batch_indices, :, :, :]\n",
        "\n",
        "        # Preprocess data (e.g., normalize, reshape)\n",
        "        mri_images = preprocess_data(mri_images, image_size, c_dim)\n",
        "        mri_labels = preprocess_data(mri_labels, label_size, c_dim)\n",
        "        ct_images = preprocess_data(ct_images, image_size, c_dim)\n",
        "        ct_labels = preprocess_data(ct_labels, label_size, c_dim)\n",
        "\n",
        "        yield mri_images, mri_labels, ct_images, ct_labels\n",
        "\n",
        "    # Close HDF5 files\n",
        "    mri_file.close()\n",
        "    ct_file.close()"
      ],
      "metadata": {
        "id": "O--dn0t_eHkW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fusion_model(inputs):\n",
        "    x = Conv2D(256, 5, padding='same', kernel_initializer=TruncatedNormal(stddev=1e-3))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(128, 5, padding='same', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(64, 3, padding='same', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(32, 3, padding='same', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(1, 1, padding='same', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    outputs = tf.nn.tanh(x)\n",
        "    return outputs\n",
        "\n",
        "def discriminator(inputs):\n",
        "    x = Conv2D(32, 3, strides=2, padding='valid', kernel_initializer=TruncatedNormal(stddev=1e-3))(inputs)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(64, 3, strides=2, padding='valid', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(128, 3, strides=2, padding='valid', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(256, 3, strides=2, padding='valid', kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1, kernel_initializer=TruncatedNormal(stddev=1e-3))(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9o9Ry0V2W_CI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shapes\n",
        "image_size = 256\n",
        "label_size = 256\n",
        "c_dim = 1\n",
        "\n",
        "# Define placeholders\n",
        "images_mri = Input(shape=(image_size, image_size, c_dim), name='images_mri')\n",
        "labels_mri = Input(shape=(label_size, label_size, c_dim), name='labels_mri')\n",
        "images_ct = Input(shape=(image_size, image_size, c_dim), name='images_ct')\n",
        "labels_ct = Input(shape=(label_size, label_size, c_dim), name='labels_ct')\n",
        "\n",
        "# Concatenate inputs\n",
        "input_image = tf.concat([images_mri, images_ct], axis=-1)\n",
        "\n",
        "# Pass through fusion model\n",
        "fusion_image = fusion_model(input_image)\n",
        "\n",
        "# Pass through discriminator\n",
        "pos = discriminator(labels_ct)\n",
        "neg = discriminator(fusion_image)\n",
        "\n",
        "\n",
        "# Define loss functions\n",
        "def pos_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - tf.random.uniform(shape=[tf.shape(y_pred)[0], 1], minval=0.7, maxval=1.2)))\n",
        "\n",
        "def neg_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - tf.random.uniform(shape=[tf.shape(y_pred)[0], 1], minval=0, maxval=0.3)))\n",
        "\n",
        "def d_loss(y_true, y_pred):\n",
        "    return pos_loss(y_true, y_pred) + neg_loss(y_true, y_pred)\n",
        "\n",
        "def g_loss_1(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - tf.random.uniform(shape=[tf.shape(y_pred)[0], 1], minval=0.7, maxval=1.2)))\n",
        "\n",
        "def g_loss_2(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) + 5 * tf.reduce_mean(tf.square(tf.image.image_gradients(y_pred) - tf.image.image_gradients(y_true)))\n",
        "\n",
        "def g_loss_total(y_true, y_pred):\n",
        "    return g_loss_1(y_true, y_pred) + 100*g_loss_2(y_true, y_pred)\n",
        "\n",
        "# Optimizers\n",
        "d_optimizer = Adam(learning_rate=0.0002)\n",
        "g_optimizer = Adam(learning_rate=0.0002)"
      ],
      "metadata": {
        "id": "yfxkQb7GXFKg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile models\n",
        "\n",
        "discriminator_model = Model(inputs=[labels_ct, fusion_image], outputs= [pos, neg])\n",
        "discriminator_model.compile(optimizer=d_optimizer, loss=None)\n",
        "\n",
        "fusion_model_input = Input(shape=(image_size, image_size, 2 * c_dim))\n",
        "fusion_output = fusion_model(fusion_model_input)\n",
        "fusion_model = Model(inputs=fusion_model_input, outputs=fusion_output)\n",
        "fusion_model.compile(optimizer=g_optimizer, loss=None)\n",
        "\n",
        "# Define train_step function\n",
        "@tf.function\n",
        "def train_step(images_mri, labels_mri, images_ct, labels_ct):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        fusion_output = fusion_model(tf.concat([images_mri, images_ct], axis=-1), training=True)\n",
        "        real_output = discriminator_model([labels_ct, tf.ones_like(labels_ct)], training=True)\n",
        "        fake_output = discriminator_model([fusion_output, tf.zeros_like(fusion_output)], training=True)\n",
        "\n",
        "        gen_loss = g_loss_total([labels_mri], fusion_output)\n",
        "        disc_loss = d_loss([labels_ct, tf.zeros_like(fusion_output)], fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, fusion_model.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_model.trainable_variables)\n",
        "\n",
        "    g_optimizer.apply_gradients(zip(gradients_of_generator, fusion_model.trainable_variables))\n",
        "    d_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_model.trainable_variables))\n",
        "\n",
        "    return disc_loss, gen_loss"
      ],
      "metadata": {
        "id": "FOKSfiJqbTor"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "data_dir_mri = 'path/to/mri/data'\n",
        "data_dir_ct = 'path/to/ct/data'\n",
        "data_loader_gen = data_loader(data_dir_mri, data_dir_ct, batch_size, image_size, label_size, c_dim)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for images_mri, labels_mri, images_ct, labels_ct in data_loader_gen:\n",
        "        disc_loss, gen_loss = train_step(images_mri, labels_mri, images_ct, labels_ct)\n",
        "        print(f\"Epoch {epoch+1}, Disc Loss: {disc_loss}, Gen Loss: {gen_loss}\")\n",
        "\n",
        "# Save models\n",
        "fusion_model.save('fusion_model.h5')\n",
        "discriminator_model.save('discriminator_model.h5')"
      ],
      "metadata": {
        "id": "ZqgkVtpNbXE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}